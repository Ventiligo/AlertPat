import psutil
import pandas as pd
from sklearn.cluster import DBSCAN
from datetime import datetime, timedelta
import logging
import pickle
import os

DATA_FILE = "metrics.pkl" 
LOG_FILE = "monitor.log" 

logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)

class MetricsStorage:
    def __init__(self):
        self.df = pd.DataFrame(columns=['timestamp', 'cpu', 'ram', 'disk', 'network'])
        try:
            self.load_data()
        except (FileNotFoundError, EOFError, pickle.PickleError) as e:
            logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ {DATA_FILE}: {e}. –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –ø—É—Å—Ç–æ–π DataFrame.")
            self.df = pd.DataFrame(columns=['timestamp', 'cpu', 'ram', 'disk', 'network'])
            self.save_data()

    def add_metrics(self):
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∏—Ö –≤ DataFrame.
        """
        new_data = {
            'timestamp': datetime.now(),
            'cpu': psutil.cpu_percent(),
            'ram': psutil.virtual_memory().percent,
            'disk': psutil.disk_usage('/').percent,
            'network': psutil.net_io_counters().bytes_sent + psutil.net_io_counters().bytes_recv
        }
        self.df = pd.concat([self.df, pd.DataFrame([new_data])], ignore_index=True)
        self.clean_data()
        self.save_data()

    def clean_data(self):
        cutoff = datetime.now() - timedelta(days=30)
        self.df = self.df[self.df['timestamp'] >= cutoff]

    def save_data(self):
        try:
            with open(DATA_FILE, 'wb') as f:
                pickle.dump(self.df, f)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ {DATA_FILE}: {e}")

    def load_data(self):
        if os.path.exists(DATA_FILE) and os.path.getsize(DATA_FILE) > 0:
            with open(DATA_FILE, 'rb') as f:
                self.df = pickle.load(f)
        else:
            raise FileNotFoundError(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö {DATA_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç.")


storage = MetricsStorage()

def detect_anomalies():

    if len(storage.df) < 10:
        logging.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π (—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 10 –∑–∞–ø–∏—Å–µ–π).")
        return None

    last_24h = storage.df[storage.df['timestamp'] >= datetime.now() - timedelta(hours=240)]
    if len(last_24h) < 5:
        logging.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π (—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 5 –∑–∞–ø–∏—Å–µ–π).")
        return None

    X = last_24h[['cpu', 'ram', 'network']].values

    try:
        model = DBSCAN(eps=2.5, min_samples=3).fit(X)
        anomalies = last_24h[model.labels_ == -1]

        return anomalies.iloc[-1] if not anomalies.empty else None
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π: {e}")
        return None

def generate_and_log_report():
    storage.add_metrics()
    last = storage.df.iloc[-1]

    msg = (
        "üìä –û—Ç—á—ë—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–µ—Ä–≤–µ—Ä–∞:\n"
        f"–í—Ä–µ–º—è: {last['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\n"
        f"CPU: {last['cpu']}%\n"
        f"RAM: {last['ram']}%\n"
        f"–î–∏—Å–∫: {last['disk']}%"
    )

    if anomaly := detect_anomalies():
        msg += (
            "\n\n‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∞–Ω–æ–º–∞–ª–∏—è!\n"
            f"–í—Ä–µ–º—è: {anomaly['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"CPU: {anomaly['cpu']}%\n"
            f"RAM: {anomaly['ram']}%"
        )

    logging.info(msg)
    print(msg)       

def get_overall_statistics():
    if not storage.df.empty:
        stats = storage.df.describe().to_string()
        full_stats_msg = f"üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥:\n{stats}"
        logging.info(full_stats_msg)
        print("\n" + full_stats_msg)
    else:
        no_stats_msg = "üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: –Ω–µ—Ç —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."
        logging.info(no_stats_msg)
        print("\n" + no_stats_msg)

if __name__ == "__main__":
    print("–ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–µ—Ä–≤–µ—Ä–∞...")

    generate_and_log_report()
    get_overall_statistics()

    print(f"\n–î–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ —Ñ–∞–π–ª–µ: {DATA_FILE}")
    print(f"–ü–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ —Ñ–∞–π–ª–µ: {LOG_FILE}")
    print("–°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É. –î–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—Å—Ç–∏—Ç–µ –µ–≥–æ —Å–Ω–æ–≤–∞.")